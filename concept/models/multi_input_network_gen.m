addpath("classes","functions","models");
clear_env();

%% define variables

datafile = "BTCUSDT.txt";

window_size = 90;
rate = 30;
no_samples = 1e4;
no_sets = 10;
prediction_length = 0;
time_before = 7*24*30;
time_after = 1*24*60;
confidence_interval = 0.5;
heuristic_limit = 0;
valid_split = 0.2;

batch_size = 512;
%adam options? 
learn_rate = 1e-3;
max_epochs = 30;

%% define model
%{
    9/9/2021
    code generated by deepnetworkdesigner
%}
lgraph = layerGraph();

tempLayers = [
    sequenceInputLayer(1,"Name","sequence_2")
    fullyConnectedLayer(256,"Name","fc_4")
    dropoutLayer(0.2,"Name","dropout_3")
    bilstmLayer(128,"Name","bilstm_3")];
lgraph = addLayers(lgraph,tempLayers);

tempLayers = [
    sequenceInputLayer(1,"Name","sequence_1")
    fullyConnectedLayer(256,"Name","fc_1")
    dropoutLayer(0.2,"Name","dropout_1")
    bilstmLayer(128,"Name","bilstm_1")];
lgraph = addLayers(lgraph,tempLayers);

tempLayers = [
    additionLayer(2,"Name","addition")
    groupNormalizationLayer("channel-wise","Name","groupnorm")
    fullyConnectedLayer(256,"Name","fc_2")
    dropoutLayer(0.2,"Name","dropout_2")
    bilstmLayer(128,"Name","bilstm_2")
    fullyConnectedLayer(1,"Name","fc_3")
    %regressionLayer("Name","regressionoutput")
    ];
lgraph = addLayers(lgraph,tempLayers);

% clean up helper variable
clear tempLayers;

lgraph = connectLayers(lgraph,"bilstm_3","addition/in1");
lgraph = connectLayers(lgraph,"bilstm_1","addition/in2");

net = dlnetwork(lgraph);


%%  training loop
models = cell(no_sets,1);

for s = 1:no_sets
    [train_samples,eval_samples] = get_samples(datafile,1,rate,time_before,time_after);
    [xData,yData] = subsample(train_samples{1},no_samples,window_size+1,prediction_length);
    [xVal,yVal] = subsample(eval_samples{1},valid_split*no_samples,window_size+1,prediction_length);
    
    gc = 1;
    [avg_g,avg_sqg] = deal([],[]);    
    tic;
    
    for epoch = 1:max_epochs
        
        locs = randperm(no_samples);
        
        for i = 1:batch_size:no_samples
            gc = gc+1;
            batch = [i:i+batch_size-1];
            batch(batch>no_samples) = [];
            
            xBatch = prepare_batch(xData(locs(batch)),'CTB');
            yBatch = prepare_batch(yData(locs(batch)),'CTB');
            
            [gradients,loss,net] = dlfeval(@model_gradients,net,xBatch,yBatch);
            [net,avg_g,avg_sqg] = adamupdate(net,gradients,avg_g,avg_sqg,gc,learn_rate);      
        end
        
        fprintf("epoch:\t%d, time_elapsed:\t%.2fs\tloss:\t%.2f\n",epoch,toc,loss);        
    end
    fprintf("\nmodel evaluation\n\n");
    evaluate_model(...
        eval_samples{1},net,window_size,prediction_length,false,false,0,confidence_interval,heuristic_limit);
    models{s} = net;    
    fprintf("\n");
end

%% helper functions 

function batch = prepare_batch(data,labels)
    batch = cell2mat(data);
    batch = reshape(batch,[1, size(batch,2), size(batch,1)]);
    batch = gpudl(batch,labels);
end

function data = gpudl(data,labels)
    data = gpuArray(dlarray(data,labels));  
end

function [gradients,loss,network] = model_gradients(network,xBatch,yBatch)
    
    y = forward(network,xBatch(:,:,2:end),yBatch(:,:,1:end-1));
    loss = sqrt(sum(power(y-yBatch(:,:,2:end),2),'all')/numel(y));

    gradients = dlgradient(loss,network.Learnables);
end











